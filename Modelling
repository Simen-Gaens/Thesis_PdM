#Model development
#Import initial packages and the dataset
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import sklearn as sk
import matplotlib as plt
import seaborn as sns

data = pd.read_csv('Dataset.csv', delimiter=";", decimal=",", encoding="utf-8")

data.head()
data.info()

#############################

#Ensure int/float features are numeric
data['Average_Voor']=pd.to_numeric(data['Average_Voor'], errors='coerce')
data['difference_voor_na']=pd.to_numeric(data['difference_voor_na'], errors='coerce') 
data['volatility_left_sensor']=pd.to_numeric(data['volatility_left_sensor'], errors='coerce') 
data['volatility_right_sensor']=pd.to_numeric(data['volatility_left_sensor'], errors='coerce') 
data['volatility_after']=pd.to_numeric(data['volatility_left_sensor'], errors='coerce') 

#Ensure correct dtypes
data['Average_Voor'] = data['Average_Voor'].astype("int64")
data['difference_voor_na'] = data['difference_voor_na'].astype("int64")
data['volatility_left_sensor'] = data['volatility_left_sensor'].astype("float64")
data['abovetreshhold'] = data['abovetreshhold'].astype("category")
data['corrected state'] = data['corrected state'].astype("category")
data['boards_till_mediate'] = data['boards_till_mediate'].astype("category")
data['boards_till_bad'] = data['boards_till_bad'].astype("category")

#Make categoricals into their cat.codes so we can model on them
data['corrected state'] = data['corrected state'].cat.codes
data['abovetreshhold'] = data['abovetreshhold'].cat.codes

##############################
#Check Null values
data.isnull().sum()


def Fix_Null(feature):
    fill_value= feature.mode()[0]
    feature.fillna(fill_value, inplace = True)

Fix_Null(data['volatility_left_sensor'])
Fix_Null(data['volatility_right_sensor'])
Fix_Null(data['volatility_after'])



data.describe()
data.info()

############################
#Check division of target variables
print(data.abovetreshhold.value_counts(normalize=True))
data.abovetreshhold.value_counts(normalize=True).plot.pie()

print(data.boards_till_mediate.value_counts(normalize=True))
data.boards_till_mediate.value_counts(normalize=True).plot.pie()

print(data.boards_till_bad.value_counts(normalize=True))
data.boards_till_bad.value_counts(normalize=True).plot.pie()

############################
#Plotting
sns.pairplot(data = data, vars=['difference_voor_na','difference_after_links_rechts','abovetreshhold'])
sns.pairplot(data = data, vars=['volatility_after','boards_till_bad'])

sns.pairplot(data = data, vars=['min_max_left_right_in_window_10','min_max_voor_na_window_10','volatility_after','boards_till_bad'])
sns.pairplot(data = data, vars=['min_max_left_right_in_window_10','min_max_voor_na_window_10','volatility_after','boards_till_mediate'])

#Correlation plotting
sns.heatmap(data.corr(), annot=True, cmap = 'Reds')
data.corr()

############################
#Correlation and covariance of target
from numpy import cov
covariance= cov(data['boards_till_mediate'],data['boards_till_bad'])
covariance

from scipy.stats import pearsonr
corr= pearsonr(data['boards_till_mediate'],data['boards_till_bad'])
corr

############################
#Volatility after/left/right sensor are highly correlated, as well as Average recht/links voor
#Groupby to see if initial 
data.groupby('abovetreshhold')['difference_voor_na'].mean()

data.groupby('boards_till_mediate')['difference_voor_na'].mean() #Rather intuitive except first category
data.groupby('boards_till_bad')['difference_voor_na'].mean() #Seems to be a lot less intuitive

data.groupby('boards_till_mediate')['difference_after_links_rechts'].mean()
data.groupby('boards_till_bad')['difference_after_links_rechts'].mean() #Median makes more sense

data.groupby('boards_till_mediate')['volatility_after'].mean()
data.groupby('boards_till_bad')['volatility_after'].mean()

data.groupby('boards_till_mediate')['average_diff_window_10_voor_na'].mean()
data.groupby('boards_till_bad')['average_diff_window_10_voor_na'].mean()

data.groupby('boards_till_mediate')['min_max_voor_na_window_10'].mean()
data.groupby('boards_till_bad')['min_max_voor_na_window_10'].mean()

##########################

#Board till plots
sns.boxplot(data['boards_till_mediate'], data['difference_after_links_rechts'])
sns.boxplot(data['boards_till_mediate'], data['difference_voor_na'])

sns.boxplot(data['boards_till_bad'], data['difference_after_links_rechts'])
sns.boxplot(data['boards_till_bad'], data['difference_voor_na'])

sns.boxplot(data['boards_till_mediate'], data['min_max_voor_na_window_10'])
sns.boxplot(data['boards_till_mediate'], data['min_max_left_right_in_window_10'])

sns.boxplot(data['boards_till_bad'], data['min_max_voor_na_window_10'])
sns.boxplot(data['boards_till_bad'], data['min_max_left_right_in_window_10'])

################################
#First variable in initial model
sns.boxplot(data['abovetreshhold'], data['Average_Voor'])
sns.boxplot(data['abovetreshhold'], data['volatility_after'])
sns.boxplot(data['abovetreshhold'], data['difference_after_links_rechts'])
sns.boxplot(data['abovetreshhold'], data['difference_voor_na'])
sns.boxplot(data['abovetreshhold'], data['boards_till_mediate'])
sns.boxplot(data['abovetreshhold'], data['boards_till_bad'])
sns.boxplot(data['abovetreshhold'], data['average_diff_window_10_Left_right'])
sns.boxplot(data['abovetreshhold'], data['min_max_left_right_in_window_10'])
sns.boxplot(data['abovetreshhold'], data['min_max_voor_na_window_10'])

################################
#Deal with extreme Values
from scipy import stats

def limit_extreme(df):
    z=np.abs(stats.zscore(df))
    data_clean=data[(z< 3)]
    return data_clean

limit_extreme(data['Average_Voor'])  
limit_extreme(data['difference_voor_na'])
limit_extreme(data['Average_Rechts_Voor'])
limit_extreme(data['difference_after_links_rechts'])  
limit_extreme(data['difference_voor_na'])  
limit_extreme(data['volatility_after'])  
limit_extreme(data['average_diff_window_10_Left_right'])  
limit_extreme(data['min_max_left_right_in_window_10'])  
limit_extreme(data['min_max_voor_na_window_10'])  
limit_extreme(data['volatility_right_sensor'])  
limit_extreme(data['volatility_left_sensor'])  
limit_extreme(data['difference_voor_na'])  

#############################
#Delete unnecessary features

try:
    del data['Average_Na']
    del data['Average_Rechts_Achter']
    del data['Average_Links_Achter']
    del data['Average_Rechts_Voor']
    del data['Average_Links_Voor']
    del data['volatility_left_sensor']
    del data['volatility_right_sensor']
    del data['Board_Number']
    del data['corrected state']
    del data['Average_Voor']
except:
    pass
    
################################
#Train test splits
y1_data = data['boards_till_mediate']
y2_data = data['boards_till_bad'] 
data1=data.drop('boards_till_mediate', inplace=False, axis=1)

data2=data1.drop('boards_till_bad', inplace=False, axis=1)
data1['boards_till_mediate'] = data['boards_till_mediate'].cat.codes



X1_train, X1_test, y1_train, y1_test = train_test_split(data1, y1_data, test_size=0.3, random_state=1)
X2_train, X2_test, y2_train, y2_test = train_test_split(data2, y2_data, test_size=0.3, random_state=1)

###########################
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score

#RF boards_till_mediate
rf1=RandomForestClassifier(n_estimators=100, random_state=42,bootstrap=True, oob_score=True)
rf1.fit(X1_train, y1_train)
rf1.score(X1_test, y1_test)

y1_pred=rf1.predict(X1_test)
cf1=confusion_matrix(y1_test, y1_pred)
sns.heatmap(cf1/np.sum(cf1), annot=True, 
            fmt='.2%', cmap='Blues')


f1_score(y1_test, y1_pred, average='weighted')

rf1.get_params()
##########################
#RF boards_till_bad
rf2=RandomForestClassifier(n_estimators=100, random_state=1, bootstrap=True, oob_score=True)
rf2.fit(X2_train, y2_train)

rf2.score(X2_train, y2_train)
rf2.score(X2_test, y2_test)

y2_pred=rf2.predict(X2_test)
cf2=confusion_matrix(y2_test, y2_pred)
sns.heatmap(cf2/np.sum(cf2), annot=True, 
            fmt='.2%', cmap='Blues')


f1_score(y2_test, y2_pred, average='weighted')

#############################
from xgboost import XGBClassifier

#XG model boards_till_mediate
XG1 = XGBClassifier(use_label_encoder=True, eval_metric='mlogloss', sampling_method='gradient_based')
XG1.fit(X1_train, y1_train)

XG1.score(X1_train, y1_train)
XG1.score(X1_test, y1_test)

y3_pred=XG1.predict(X1_test)
cf3=confusion_matrix(y1_test, y3_pred)
sns.heatmap(cf3/np.sum(cf3), annot=True, 
            fmt='.2%', cmap='Blues')


f1_score(y1_test, y3_pred, average='weighted')

#############################
#XG model boards_till_bad
XG2 = XGBClassifier(use_label_encoder=True, eval_metric='mlogloss',sampling_method='gradient_based')
XG2.fit(X2_train, y2_train)

XG2.score(X2_train, y2_train)
XG2.score(X2_test, y2_test)

y4_pred=XG2.predict(X2_test)
cf4=confusion_matrix(y1_test, y1_pred)
sns.heatmap(cf4/np.sum(cf4), annot=True, 
            fmt='.2%', cmap='Blues')


f1_score(y2_test, y4_pred, average='weighted')



##############################################################

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import uniform, truncnorm, randint

#Random search
model_params = {
    # randomly sample numbers from 4 to 204 estimators
    'n_estimators': randint(4,200),
    # normally distributed max_features, with mean .25 stddev 0.1, bounded between 0 and 1
    'max_features': randint(3,10),
    'min_samples_split': randint(1,10),
    'max_depth' : uniform(4,30)
}

rf_model = RandomForestClassifier()
clf = RandomizedSearchCV(rf_model, model_params, n_iter=60, cv=5, random_state=1)
rf_model=clf.fit(X1_train, y1_train)

from pprint import pprint
pprint(rf_model.best_estimator_.get_params())
#################################################

#60 iterations
rf_rand=RandomForestClassifier(n_estimators=38, max_features=9, min_samples_split= 4, min_samples_leaf=1, max_depth = 33.10)
rf_rand.fit(X1_train, y1_train)

rf_rand.score(X1_train, y1_train)
rf_rand.score(X1_test, y1_test)

yrand_pred=rf_rand.predict(X1_test)
cfrand=confusion_matrix(y1_test, yrand_pred)
sns.heatmap(cfrand/np.sum(cfrand), annot=True, 
            fmt='.2%', cmap='Blues')


f1_score(y1_test, yrand_pred, average='weighted')

#100 iterations

model_params = {
    # randomly sample numbers from 4 to 204 estimators
    'n_estimators': randint(4,200),
    # normally distributed max_features, with mean .25 stddev 0.1, bounded between 0 and 1
    'max_features': randint(3,10),
    'min_samples_split': randint(1,10),
    'max_depth' : uniform(4,30)
}

rf_model = RandomForestClassifier()
clf = RandomizedSearchCV(rf_model, model_params, n_iter=100, cv=5, random_state=1)
rf_model2=clf.fit(X1_train, y1_train)


rf_rand=RandomForestClassifier(n_estimators=165, max_features=7, min_samples_split= 2, min_samples_leaf=1, max_depth = 23.81)
rf_rand.fit(X1_train, y1_train)

rf_rand.score(X1_train, y1_train)
rf_rand.score(X1_test, y1_test)

yrand_pred=rf_rand.predict(X1_test)
cfrand=confusion_matrix(y1_test, yrand_pred)
sns.heatmap(cfrand/np.sum(cfrand), annot=True, 
            fmt='.2%', cmap='Blues')


f1_score(y1_test, yrand_pred, average='weighted')
###################################################
#Gridsearch

rfc=RandomForestClassifier(random_state=42)

param_grid = { 
    'n_estimators': [100,150,200],
    'max_features': [3,5,7,9],
    'max_depth' : [12,20,24,30],
    'min_samples_split': [2,5,9]
}

CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)
CV_rfc.fit(X1_train, y1_train)
CV_rfc.best_params_

###############################################
#Grid search boards_till_mediate
rf_grid=RandomForestClassifier(n_estimators=200, max_depth=30, min_samples_split=2, max_features= 9)
rf_grid.fit(X1_train, y1_train)

rf_grid.score(X1_train, y1_train)
rf_grid.score(X1_test, y1_test)

ygrid_pred=rf_grid.predict(X1_test)
cfgrid=confusion_matrix(y1_test, ygrid_pred)
sns.heatmap(cfgrid/np.sum(cfgrid), annot=True, 
            fmt='.2%', cmap='Blues')


f1_score(y1_test, ygrid_pred, average='weighted')

###################################
#Bayesian optimisation
!pip install bayesian-optimization
!pip install bayes_opt
#Importing necessary libraries
from bayes_opt import BayesianOptimization
import xgboost as xgb
from sklearn.metrics import mean_squared_error

X1_train = pd.DataFrame(X1_train)
X1_test = pd.DataFrame(X1_test)
#y1_train = pd.DataFrame(y1_train)
#y1_test = pd.DataFrame(y1_test)


X2_train = pd.DataFrame(X2_train)
X2_test = pd.DataFrame(X2_test)

#!pip install hyperopt
from hyperopt import hp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold
from sklearn.metrics import confusion_matrix,classification_report,precision_score, recall_score, f1_score, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from bayes_opt import BayesianOptimization


#5 iterations
def clean_dataset(df):
    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"
    df.dropna(inplace=True)
    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)
    return df[indices_to_keep].astype(np.float64)



clean_dataset(X1_train)
clean_dataset(X2_train)

def stratified_kfold_score(clf,X,y,n_fold):
    X,y = X.values,y.values
    strat_kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=1)
    accuracy_list = []

    for train_index, test_index in strat_kfold.split(X, y):
        x_train_fold, x_test_fold = X[train_index], X[test_index]
        y_train_fold, y_test_fold = y[train_index], y[test_index]
        clf.fit(x_train_fold, y_train_fold)
        preds = clf.predict(x_test_fold)
        accuracy_test = accuracy_score(preds,y_test_fold)
        accuracy_list.append(accuracy_test)

    return np.array(accuracy_list).mean()

def bo_params_rf(n_estimators,max_depth, max_features, min_samples_split):
    
    params = {
        'max_depth':max_depth,
        'n_estimators':int(n_estimators),
        'max_features': int(max_features),
        'min_samples_split' : int(max(min_samples_split,2)),
    }
    clf = RandomForestClassifier(max_depth=params['max_depth'],n_estimators=params['n_estimators'],
                                 min_samples_split = params['min_samples_split'])
    score = stratified_kfold_score(clf,X1_train, y1_train,5)
    return score

rf_bo = BayesianOptimization(bo_params_rf, {
                                                'max_depth':(4,30),
                                              'n_estimators':(4,200),
    'max_features': (3,9),
                                            'min_samples_split': (2,10),
                                             })

results = rf_bo.maximize(n_iter=5, init_points=8,acq='ei')
#Model
params = rf_bo.max['params']
params['n_estimators']= int(params['n_estimators'])
#print(params)
#Grid search boards_till_mediate
rf_grid=RandomForestClassifier(max_depth=params['max_depth'],n_estimators=params['n_estimators'], min_samples_split = int(params['min_samples_split']), max_features=int(params['max_features']))
rf_grid.fit(X1_train, y1_train)

rf_grid.score(X1_train, y1_train)
rf_grid.score(X1_test, y1_test)

ygrid_pred=rf_grid.predict(X1_test)
cfBO1=confusion_matrix(y1_test, ygrid_pred)
sns.heatmap(cfBO1/np.sum(cfBO1), annot=True, 
            fmt='.2%', cmap='Blues')


f1_score(y1_test, ygrid_pred, average='weighted')

#######
#60 iterations

def clean_dataset(df):
    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"
    df.dropna(inplace=True)
    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)
    return df[indices_to_keep].astype(np.float64)



clean_dataset(X1_train)
clean_dataset(X2_train)

def stratified_kfold_score(clf,X,y,n_fold):
    X,y = X.values,y.values
    strat_kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=1)
    accuracy_list = []

    for train_index, test_index in strat_kfold.split(X, y):
        x_train_fold, x_test_fold = X[train_index], X[test_index]
        y_train_fold, y_test_fold = y[train_index], y[test_index]
        clf.fit(x_train_fold, y_train_fold)
        preds = clf.predict(x_test_fold)
        accuracy_test = accuracy_score(preds,y_test_fold)
        accuracy_list.append(accuracy_test)

    return np.array(accuracy_list).mean()

def bo_params_rf(n_estimators,max_depth, max_features, min_samples_split):
    
    params = {
        'max_depth':max_depth,
        'n_estimators':int(n_estimators),
        'max_features': int(max_features),
        'min_samples_split' : int(max(min_samples_split,2)),
    }
    clf = RandomForestClassifier(max_depth=params['max_depth'],n_estimators=params['n_estimators'],
                                 min_samples_split = params['min_samples_split'])
    score = stratified_kfold_score(clf,X1_train, y1_train,5)
    return score

rf_bo = BayesianOptimization(bo_params_rf, {
                                                'max_depth':(4,30),
                                              'n_estimators':(4,200),
    'max_features': (3,9),
                                            'min_samples_split': (2,10),
                                             })

results = rf_bo.maximize(n_iter=60, init_points=8,acq='ei')

params = rf_bo.max['params']
params['n_estimators']= int(params['n_estimators'])
#print(params)
#Grid search boards_till_mediate
rf_grid=RandomForestClassifier(max_depth=params['max_depth'],n_estimators=params['n_estimators'], min_samples_split = int(params['min_samples_split']), max_features=int(params['max_features']))
rf_grid.fit(X1_train, y1_train)

rf_grid.score(X1_train, y1_train)
rf_grid.score(X1_test, y1_test)

ygrid_pred=rf_grid.predict(X1_test)
cfBO2=confusion_matrix(y1_test, ygrid_pred)
sns.heatmap(cfBO2/np.sum(cfBO2), annot=True, 
            fmt='.2%', cmap='Blues')


f1_score(y1_test, ygrid_pred, average='weighted')

#######
#100 iterations

def clean_dataset(df):
    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"
    df.dropna(inplace=True)
    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)
    return df[indices_to_keep].astype(np.float64)



clean_dataset(X1_train)
clean_dataset(X2_train)

def stratified_kfold_score(clf,X,y,n_fold):
    X,y = X.values,y.values
    strat_kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=1)
    accuracy_list = []

    for train_index, test_index in strat_kfold.split(X, y):
        x_train_fold, x_test_fold = X[train_index], X[test_index]
        y_train_fold, y_test_fold = y[train_index], y[test_index]
        clf.fit(x_train_fold, y_train_fold)
        preds = clf.predict(x_test_fold)
        accuracy_test = accuracy_score(preds,y_test_fold)
        accuracy_list.append(accuracy_test)

    return np.array(accuracy_list).mean()

def bo_params_rf(n_estimators,max_depth, max_features, min_samples_split):
    
    params = {
        'max_depth':max_depth,
        'n_estimators':int(n_estimators),
        'max_features': int(max_features),
        'min_samples_split' : int(max(min_samples_split,2)),
    }
    clf = RandomForestClassifier(max_depth=params['max_depth'],n_estimators=params['n_estimators'],
                                 min_samples_split = params['min_samples_split'])
    score = stratified_kfold_score(clf,X1_train, y1_train,5)
    return score

rf_bo = BayesianOptimization(bo_params_rf, {
                                                'max_depth':(4,30),
                                              'n_estimators':(4,200),
    'max_features': (3,9),
                                            'min_samples_split': (2,10),
                                             })

results = rf_bo.maximize(n_iter=100, init_points=8,acq='ei')

params = rf_bo.max['params']
params['n_estimators']= int(params['n_estimators'])
#print(params)
#Grid search boards_till_mediate
rf_grid=RandomForestClassifier(max_depth=params['max_depth'],n_estimators=params['n_estimators'], min_samples_split = int(params['min_samples_split']), max_features=int(params['max_features']))
rf_grid.fit(X1_train, y1_train)

rf_grid.score(X1_train, y1_train)
rf_grid.score(X1_test, y1_test)

ygrid_pred=rf_grid.predict(X1_test)
cfgrid=confusion_matrix(y1_test, ygrid_pred)
sns.heatmap(cfgrid/np.sum(cfgrid), annot=True, 
            fmt='.2%', cmap='Blues')
f1_score(y1_test, ygrid_pred, average='weighted')
###########################################################
#Shapley values

!pip install shap
import shap

#Boards till mediate
explainer = shap.TreeExplainer(rf1)
shap_values = explainer.shap_values(X1_train)
shap.summary_plot(shap_values, features=X1_train, feature_names=X1_train.columns)
shap.initjs()

#Boards till bad
explainer = shap.TreeExplainer(rf2)
shap_values = explainer.shap_values(X2_train)
shap.summary_plot(shap_values, features=X2_train, feature_names=X2_train.columns)
